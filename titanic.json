{"paragraphs":[{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482284352173_672684894","id":"20161221-013912_2009262650","dateCreated":"2016-12-21T01:39:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:131","text":"%pyspark\n\ntrain_path='s3://kaggletg/titanic/train.csv'\ntest_path='s3://kaggletg/titanic/test.csv'\n# Load csv file as RDD\ntrain_rdd = sc.textFile(train_path)\ntest_rdd = sc.textFile(test_path)\ntrain_rdd.take(3)","dateUpdated":"2016-12-21T01:41:39+0000","dateFinished":"2016-12-21T01:41:46+0000","dateStarted":"2016-12-21T01:41:39+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"[u'PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked', u'1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S', u'2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C']\n"},"focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482284457617_-1906733029","id":"20161221-014057_2133801748","dateCreated":"2016-12-21T01:40:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:290","dateUpdated":"2016-12-21T01:49:19+0000","dateFinished":"2016-12-21T01:49:22+0000","dateStarted":"2016-12-21T01:49:19+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"+-----------+--------+------+---------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n|PassengerId|Survived|Pclass|FirstName|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n+-----------+--------+------+---------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n|          1|       0|     3|   Braund|     Mr. Owen Harris|  male| 22|    1|    0|       A/5 21171|   7.25|     |       S|\n|          2|       1|     1|  Cumings| Mrs. John Bradle...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n|          3|       1|     3|Heikkinen|         Miss. Laina|female| 26|    0|    0|STON/O2. 3101282|  7.925|     |       S|\n+-----------+--------+------+---------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\nonly showing top 3 rows\n\n"},"text":"%pyspark\n\n# Parse RDD to DF\ndef parseTrain(rdd):\n \n    # extract data header (first row)\n    header = rdd.first()\n    # remove header\n    body = rdd.filter(lambda r: r!=header)\n    def parseRow(row):\n        # a function to parse each text row into\n        # data format\n        # remove double quote, split the text row by comma\n        row_list = row.replace('\"','').split(\",\")\n        # convert python list to tuple, which is\n        # compatible with pyspark data structure\n        row_tuple = tuple(row_list)\n        return row_tuple\n \n    rdd_parsed = body.map(parseRow)\n \n    colnames = header.split(\",\")\n    colnames.insert(3,'FirstName')\n \n    return rdd_parsed.toDF(colnames)\n \ndef parseTest(rdd):\n    header = rdd.first()\n    body = rdd.filter(lambda r: r!=header)\n \n    def parseRow(row):\n        row_list = row.replace('\"','').split(\",\")\n        row_tuple = tuple(row_list)\n        return row_tuple\n \n    rdd_parsed = body.map(parseRow)\n \n    colnames = header.split(\",\")\n    colnames.insert(2,'FirstName')\n \n    return rdd_parsed.toDF(colnames)\n \ntrain_df = parseTrain(train_rdd)\ntest_df = parseTest(test_rdd)\n\ntrain_df.show(3)"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482284546721_233395240","id":"20161221-014226_1404767846","dateCreated":"2016-12-21T01:42:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:385","dateUpdated":"2016-12-21T01:50:11+0000","dateFinished":"2016-12-21T01:50:12+0000","dateStarted":"2016-12-21T01:50:11+0000","result":{"code":"SUCCESS","type":"TEXT","msg":""},"text":"%pyspark\n\n## Add Survived column to test\nfrom pyspark.sql.functions import lit, col\ntrain_df = train_df.withColumn('Mark',lit('train'))\ntest_df = (test_df.withColumn('Survived',lit(0))\n                  .withColumn('Mark',lit('test')))\ntest_df = test_df[train_df.columns]\n## Append Test data to Train data\ndf = train_df.unionAll(test_df)"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482284923916_-504068234","id":"20161221-014843_1735993573","dateCreated":"2016-12-21T01:48:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:533","dateUpdated":"2016-12-21T01:53:53+0000","dateFinished":"2016-12-21T01:52:03+0000","dateStarted":"2016-12-21T01:52:03+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"root\n |-- PassengerId: string (nullable = true)\n |-- Survived: double (nullable = true)\n |-- Pclass: string (nullable = true)\n |-- FirstName: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: double (nullable = true)\n |-- Parch: double (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Cabin: string (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- Mark: string (nullable = false)\n\n"},"text":"%pyspark\n\n# Convert Age, SibSp, Parch, Fare to Numeric\n\ndf = (df.withColumn('Age',df['Age'].cast(\"double\"))\n            .withColumn('SibSp',df['SibSp'].cast(\"double\"))\n            .withColumn('Parch',df['Parch'].cast(\"double\"))\n            .withColumn('Fare',df['Fare'].cast(\"double\"))\n            .withColumn('Survived',df['Survived'].cast(\"double\"))\n            )\ndf.printSchema()"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482285056102_711401158","id":"20161221-015056_1225375998","dateCreated":"2016-12-21T01:50:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:659","dateUpdated":"2016-12-21T01:53:17+0000","dateFinished":"2016-12-21T01:53:41+0000","dateStarted":"2016-12-21T01:53:17+0000","result":{"code":"SUCCESS","type":"TEXT","msg":""},"text":"%pyspark\n\n# Impute missing Age and Fare with the Average\nnumVars = ['Survived','Age','SibSp','Parch','Fare']\ndef countNull(df,var):\n    return df.where(df[var].isNull()).count()\n \nmissing = {var: countNull(df,var) for var in numVars}\nage_mean = df.groupBy().mean('Age').first()[0]\nfare_mean = df.groupBy().mean('Fare').first()[0]\ndf = df.na.fill({'Age':age_mean,'Fare':fare_mean})\n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482285169014_-105131242","id":"20161221-015249_976845270","dateCreated":"2016-12-21T01:52:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:746","dateUpdated":"2016-12-21T01:55:40+0000","dateFinished":"2016-12-21T01:55:12+0000","dateStarted":"2016-12-21T01:55:04+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"+--------------------+-----+\n|                Name|Title|\n+--------------------+-----+\n|     Mr. Owen Harris|   Mr|\n| Mrs. John Bradle...|  Mrs|\n|         Miss. Laina| Miss|\n+--------------------+-----+\nonly showing top 3 rows\n\n"},"text":"%pyspark\n\n#  Extract Title from Name\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n \n## created user defined function to extract title\ngetTitle = udf(lambda name: name.split('.')[0].strip(),StringType())\ndf = df.withColumn('Title', getTitle(df['Name']))\n \ndf.select('Name','Title').show(3)"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482285304384_-1936319823","id":"20161221-015504_1565687800","dateCreated":"2016-12-21T01:55:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:863","dateUpdated":"2016-12-21T01:56:32+0000","dateFinished":"2016-12-21T01:56:47+0000","dateStarted":"2016-12-21T01:56:32+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"+--------+----------------+\n|Embarked|Embarked_indexed|\n+--------+----------------+\n|       S|             0.0|\n|       C|             1.0|\n|       S|             0.0|\n+--------+----------------+\nonly showing top 3 rows\n\n"},"text":"%pyspark\n\n# index categorical variables\n\ncatVars = ['Pclass','Sex','Embarked','Title']\n \n## index Sex variable\nfrom pyspark.ml.feature import StringIndexer\nsi = StringIndexer(inputCol = 'Sex', outputCol = 'Sex_indexed')\ndf_indexed = si.fit(df).transform(df).drop('Sex').withColumnRenamed('Sex_indexed','Sex')\n \n## make use of pipeline to index all categorical variables\ndef indexer(df,col):\n    si = StringIndexer(inputCol = col, outputCol = col+'_indexed').fit(df)\n    return si\n \nindexers = [indexer(df,col) for col in catVars]\n \nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages = indexers)\ndf_indexed = pipeline.fit(df).transform(df)\n \ndf_indexed.select('Embarked','Embarked_indexed').show(3)"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482285392320_-470705638","id":"20161221-015632_458520776","dateCreated":"2016-12-21T01:56:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:945","dateUpdated":"2016-12-21T02:10:51+0000","dateFinished":"2016-12-21T02:11:02+0000","dateStarted":"2016-12-21T02:10:51+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"+-----+-----+--------------------+-----+\n| mark|label|            features|index|\n+-----+-----+--------------------+-----+\n|train|  0.0|[22.0,1.0,0.0,7.2...|  0.0|\n|train|  1.0|[38.0,1.0,0.0,71....|  1.0|\n|train|  1.0|[26.0,0.0,0.0,7.9...|  1.0|\n+-----+-----+--------------------+-----+\nonly showing top 3 rows\n\n"},"text":"%pyspark \n\n# convert to labels/feature format\n# In order to apply ML/MLLIB, we need covert features to Vectors (either SparseVector or DenseVector).\n\ncatVarsIndexed = [i+'_indexed' for i in catVars]\nfeaturesCol = numVars+catVarsIndexed\nfeaturesCol.remove('Survived')\nlabelCol = ['Mark','Survived']\n\nfrom pyspark.sql import Row\nfrom pyspark.ml.linalg import DenseVector\nrow = Row('mark','label','features')\n\ndf_indexed = df_indexed[labelCol+featuresCol]\n# 0-mark, 1-label, 2-features\nlf = df_indexed.rdd.map(lambda r: (row(r[0], r[1],DenseVector(r[2:])))).toDF()\n# index label \nlf = StringIndexer(inputCol = 'label',outputCol='index').fit(lf).transform(lf)\n \nlf.show(3)"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482285513783_-917429759","id":"20161221-015833_327385937","dateCreated":"2016-12-21T01:58:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1031","dateUpdated":"2016-12-21T02:11:06+0000","dateFinished":"2016-12-21T02:11:11+0000","dateStarted":"2016-12-21T02:11:06+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"Train Data Number of Row: 636\nValidate Data Number of Row: 255\nTest Data Number of Row: 418\n"},"text":"%pyspark\n\n# split back train/test data\n\ntrain = lf.where(lf.mark =='train')\ntest = lf.where(lf.mark =='test')\n \n# random split further to get train/validate\ntrain,validate = train.randomSplit([0.7,0.3],seed =121)\n \nprint 'Train Data Number of Row: '+ str(train.count())\nprint 'Validate Data Number of Row: '+ str(validate.count())\nprint 'Test Data Number of Row: '+ str(test.count())"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482285942123_-1073438075","id":"20161221-020542_431338993","dateCreated":"2016-12-21T02:05:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1153","dateUpdated":"2016-12-21T02:11:45+0000","dateFinished":"2016-12-21T02:11:49+0000","dateStarted":"2016-12-21T02:11:45+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"AUC ROC of Logistic Regression model is: 0.836952368823\n"},"text":"%pyspark\n\n# logistic regression\n\nfrom pyspark.ml.classification import LogisticRegression\n \n# regPara: lasso regularisation parameter (L1)\nlr = LogisticRegression(maxIter = 100, regParam = 0.05, labelCol='index').fit(train)\n \n# Evaluate model based on auc ROC(default for binary classification)\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n \ndef testModel(model, validate = validate):\n    pred = model.transform(validate)\n    evaluator = BinaryClassificationEvaluator(labelCol = 'index')\n    return evaluator.evaluate(pred)\n \nprint 'AUC ROC of Logistic Regression model is: '+str(testModel(lr))"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482286008165_-1824559873","id":"20161221-020648_2024299060","dateCreated":"2016-12-21T02:06:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1244","dateUpdated":"2016-12-21T02:11:54+0000","dateFinished":"2016-12-21T02:12:09+0000","dateStarted":"2016-12-21T02:11:54+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"{'RandomForest': 0.8518848700967907, 'LogisticRegression': 0.8369523688232297, 'DecistionTree': 0.7700267447784004}\n"},"text":"%pyspark\n\n# decision tree and randomforest\n\nfrom pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n \ndt = DecisionTreeClassifier(maxDepth = 3, labelCol ='index').fit(train)\nrf = RandomForestClassifier(numTrees = 100, labelCol = 'index').fit(train)\n \nmodels = {'LogisticRegression':lr,\n          'DecistionTree':dt,\n          'RandomForest':rf}\n \nmodelPerf = {k:testModel(v) for k,v in models.iteritems()}\n \nprint modelPerf"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482286104996_1748388624","id":"20161221-020824_822692510","dateCreated":"2016-12-21T02:08:24+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1337","dateUpdated":"2016-12-21T02:08:35+0000","text":""}],"name":"titanic","id":"2C44UPVCY","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}